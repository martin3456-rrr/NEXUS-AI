## Rollback Procedure If deploying the new version of *payment-service* causes 500 errors:

Check the deployment history: `kubectl rollout history deployment/payment-service -n nexus-ai`

Rollback to the previous version: `kubectl rollout undo deployment/payment-service -n nexus-ai`

## Backup and Restore Procedure (PostgreSQL) Automatic backups are performed daily at 2:00 AM (CronJob).
## Manual backup:
```bash
kubectl exec -it (kubectl get pods -l app=postgres -o jsonpath="{.items[0].metadata.name}") -- pg_dump -U nexus user_db > backup_$(date +%F).sql
```
## Restore:
```bash
cat backup.sql | kubectl exec -i (POD_NAME) -- psql -U nexus user_db 
```

### Failure Scenario: Critical - AiModelStalled
**Symptoms:** The `AiModelStalled` alert is active. Users see 500 errors when attempting predictions.
**Diagnosis:**
1. Check the `ai-analytics-service` pod logs:
   `kubectl logs -l app=ai-analytics-service -n nexus-ai --tail=100`
2. Look for `DeepLearning4j` or `OutOfMemoryError` errors.
3. Verify that the `.zip` model is loading correctly from the volume.
   **Corrective Actions:**
1. If this is OOM: Increase the memory limit in `ai-analytics-service.yml` to 2Gi and reload the deployment. 2. If the model is corrupted: Force retraining by manually invoking the endpoint (if a backdoor exists) or restore the previous model from a backup.

### Emergency Scenario: Warning - HighRequestLatency
**Symptoms:** p95 latency > 1.5s.
**Actions:**
1. Check the Grafana Dashboard "Trace View" (Zipkin).
2. Identify whether the latency is being generated by the database (SQL Slow Query) or the AI ​​calculations.
3. If the database: Check the indexes in PostgreSQL.
4. If AI: Scale horizontally: `kubectl scale deployment ai-analytics-service --replicas=3 -n nexus-ai`.

## Disaster Recovery (DR) Procedures

### Scenario 1: AI Model Corruption
**Symptoms:** `AiAnalyticsService` returns consistent 500 errors or nonsensical predictions (MSE > 10.0).
**Procedure:**
1.  Identify the last working model version artifact ID from the Artifact Registry / S3.
2.  Trigger a manual rollback pipeline via GitHub Actions:
    `gh workflow run rollback-model.yml --field version=v1.2.4`
3.  Verify restoration by hitting the health check endpoint:
    `curl http://ai-analytics-service:8082/actuator/health` (Look for "modelStatus": "UP").

### Scenario 2: Kafka Broker Failure (Data Loss Risk)
**Symptoms:** `PaymentService` cannot publish events; `NotificationService` stops sending emails.
**Procedure:**
1.  Check Zookeeper status (Leader election issues).
2.  If data corruption is suspected, switch `PaymentService` to "Synchronous Mode" (config flag `payment.kafka.enabled=false`) to process payments directly via DB, bypassing notifications temporarily to save revenue.
3.  Rebuild Kafka cluster from PVC snapshots.

### Scenario 3: Database Point-in-Time Recovery (PITR)
**Context:** Accidental deletion of user data.
**Procedure:**
1.  Stop `user-service` to prevent new writes: `kubectl scale deploy user-service --replicas=0`.
2.  Retrieve the WAL logs from the S3 backup bucket.
3.  Execute restore script provided in `infrastructure/db/restore_pitr.sh`.
4.  Validate data integrity on a staging environment before switching traffic back.

## Secret Rotation Policy and Security

### 1. Secret Management (Vault Integration)
The system uses HashiCorp Vault to dynamically manage secrets, eliminating the problem of hard-coded passwords in configuration files.

**Rotation Policy:**
* **Database Credentials:**
* Vault is configured with a database backend (`database-secrets-engine`) that dynamically generates database users per pod/instance.
* Credentials (user/password) are temporary and expire automatically every **24 hours** (TTL=24 hours).
* Rotation occurs upon pod restart or automatically via the Vault Agent Injector.

### 2. Token Security (JWT)
The rotation of cryptographic keys used to sign authorization tokens is fully automated.

* **Master Key (`jwt.secret`):** This is stored in Vault and changed systemically every **30 days**. * **Grace Period:** The application implements the "Dual Verification" mechanism. * Upon rotation, the old key is moved to the `jwt.secret.rotation` parameter. * The `JwtTokenProvider` accepts tokens signed with the new key AND tokens signed with the old key for **24 hours** after rotation. * This prevents active users from being logged out when the master key is changed.